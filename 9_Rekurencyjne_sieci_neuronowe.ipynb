{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 1: </p>\n",
        "Zaimplementuj \"zwykłą\" sieć rekurencyjną."
      ],
      "metadata": {
        "id": "8oYLZGZYP2hU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6i6jtKnMsm3"
      },
      "outputs": [],
      "source": [
        "# imports \n",
        "import torch\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "import numpy as np\n",
        "from typing import Tuple, Optional, List\n",
        "\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "all_letters = string.ascii_letters\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "\n",
        "class ListDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, data, targets):\n",
        "        \n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        \n",
        "    def __getitem__(self, ind):\n",
        "        \n",
        "        return self.data[ind], self.targets[ind]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    \n",
        "def unicode_to__ascii(s: str) -> str:\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'\n",
        "                                                                 and c in all_letters)\n",
        "                   \n",
        "\n",
        "def read_lines(filename: str) -> List[str]:\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicode_to__ascii(line) for line in lines]\n",
        "\n",
        "\n",
        "def letter_to_index(letter: str) -> int:\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "\n",
        "def line_to_tensor(line: str) -> torch.Tensor:\n",
        "    tensor = torch.zeros(len(line), n_letters)\n",
        "    for i, letter in enumerate(line):\n",
        "        tensor[i][letter_to_index(letter)] = 1\n",
        "    return tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzfrl0t5lShX",
        "outputId": "9962d278-04e7-42e4-fd95-21a6391e5b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-12 13:45:48--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.226.52.128, 13.226.52.51, 13.226.52.90, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.226.52.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip.1’\n",
            "\n",
            "data.zip.1          100%[===================>]   2.75M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-12-12 13:45:49 (21.4 MB/s) - ‘data.zip.1’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "replace data/eng-fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 input_size: int,\n",
        "                 hidden_size: int, \n",
        "                 output_size: int):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "            Dimensionality of the input vector\n",
        "        :param hidden_size: int\n",
        "            Dimensionality of the hidden space\n",
        "        :param output_size: int\n",
        "            Desired dimensionality of the output vector\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.input_to_hidden = input_size #powinno być input_to_hidden, ale wtedy nie bierze mi zmiennej pod uwagę\n",
        "        \n",
        "        self.hidden_to_output = output_size #analogicznie jak wyżej\n",
        "    # for the sake of simplicity a single forward will process only a single timestamp \n",
        "    def forward(self, \n",
        "                input: torch.tensor, \n",
        "                hidden: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
        "        \"\"\"\n",
        "        :param input: torch.tensor \n",
        "            Input tesnor for a single observation at timestep t\n",
        "            shape [batch_size, input_size]\n",
        "        :param hidden: torch.tensor\n",
        "            Representation of the memory of the RNN from previous timestep\n",
        "            shape [batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        \n",
        "        combined = torch.cat([input, hidden], dim=1) \n",
        "        hidden = self.input_to_hidden(combined)\n",
        "        output = self.hidden_to_output(hidden)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns initial value for the hidden state\n",
        "        \"\"\"\n",
        "        return torch.zeros(batch_size, self.hidden_size, requires_grad=True)\n"
      ],
      "metadata": {
        "id": "PSXzytZiQGKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_class = len(label_to_idx)\n",
        "\n",
        "# initialize network and optimizer\n",
        "rnn = RNN(n_letters, 256, n_class)\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.01)   \n",
        "\n",
        "# we will train for only a single epoch \n",
        "epochs = 1\n",
        "\n",
        "\n",
        "# main loop\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    loss_buffer = []\n",
        "    \n",
        "    for i, (x, y) in enumerate(train_loader):  \n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        # get initial hidden state\n",
        "        hidden = rnn.init_hidden(x.shape[0])\n",
        "        \n",
        "        # get output for the sample, remember that we treat it as a sequence\n",
        "        # so you need to iterate over the 2nd, time dimensiotn\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "        \n",
        "        \n",
        "        #najlepiej w pętli; uwaga RNN przy okazji generuje hidden\n",
        "        for i in range(epochs):\n",
        "          total_loss = 0\n",
        "          hidden = Variable(torch.zeros((1, hidden_size)).type(dtype),\n",
        "        requires_grad=True)\n",
        "        \n",
        "        loss = cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()  \n",
        "        \n",
        "        loss_buffer.append(loss.item())\n",
        "        \n",
        "        if i % 1000 == 1:\n",
        "            print(f\"Epoch: {epoch} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\")\n",
        "            loss_buffer = []\n",
        "    \n",
        "\n",
        "# evaluate on the test set\n",
        "with torch.no_grad():\n",
        "    ps = []\n",
        "    ys = []\n",
        "    correct = 0\n",
        "    for i, (x, y) in enumerate(test_loader):\n",
        "        ys.append(y.numpy())\n",
        "\n",
        "        hidden = rnn.init_hidden(x.shape[0])\n",
        "        seq_len = x.shape[1]\n",
        "        \n",
        "        output = rnn(input[0], hidden)\n",
        "\n",
        "        pred = output.argmax(dim=1)\n",
        "        ps.append(pred.cpu().numpy())\n",
        "    \n",
        "    ps = np.concatenate(ps, axis=0)\n",
        "    ys = np.concatenate(ys, axis=0)\n",
        "    f1 = f1_score(ys, ps, average='weighted')\n",
        "    \n",
        "    print(f\"Final F1 score: {f1:.2f}\")\n",
        "    assert f1 > 0.15, \"You should get over 0.15 f1 score, try changing some hyperparams!\""
      ],
      "metadata": {
        "id": "PhTRqPu8nU98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 2. </p>\n",
        "Zaimplementuj funkcje predict, która przyjmuje nazwisko w postaci stringa oraz model RNN i wypisuje 3 najlepsze predykcje narodowości dla tego nazwiska razem z ich logitami.\n",
        "\n",
        "Hint: Przyda się tutaj jedna z funkcji z pierwszej komórki notebooka."
      ],
      "metadata": {
        "id": "inht1gvfkII7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input_line, n_predictions=3): #do 3 najlepszych\n",
        "    print('\\n> %s' % input_line)\n",
        "    with torch.no_grad():\n",
        "        output = evaluate(lineToTensor(input_line))\n",
        "\n",
        "        # wybór najlepszej kategorii\n",
        "        topv, topi = output.topk(n_predictions, 1, True)\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(n_predictions):\n",
        "            value = topv[0][i].item()\n",
        "            category_index = topi[0][i].item()\n",
        "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "            predictions.append([value, all_categories[category_index]]) \n",
        "\n",
        "#próbowałam jeszcze z def line_to_tensor, ale utraciłam wynik"
      ],
      "metadata": {
        "id": "reBBj31zkOaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n",
        "\n",
        "for name in some_names:\n",
        "    print(name)\n",
        "    predict(name, rnn)"
      ],
      "metadata": {
        "id": "uMBfebkdkXab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 3 </p>\n",
        "Ostatnim zadaniem jest implementacji komórki i sieci LSTM."
      ],
      "metadata": {
        "id": "K1_xzbC7tXkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMCell(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 input_size: int, \n",
        "                 hidden_size: int):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "            Dimensionality of the input vector\n",
        "        :param hidden_size: int\n",
        "            Dimensionality of the hidden space\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTMCell, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # initialize LSTM weights \n",
        "        # NOTE: there are different approaches that are all correct \n",
        "        # (e.g. single matrix for all input opperations), you can pick\n",
        "        # whichever you like for this task\n",
        "    \n",
        "        w_h = hidden_size * hidden_size  #wagi dla stanu ukrytego\n",
        "        w_x = hidden_size * input_size   #macierz wag dla danych wejściowych\n",
        "\n",
        "    def forward(self, \n",
        "                input: torch.tensor, \n",
        "                states: Tuple[torch.tensor, torch.tensor]) -> Tuple[torch.tensor, torch.tensor]:\n",
        "        \n",
        "        hidden, cell = states\n",
        "        \n",
        "        # Compute input, forget, and output gates\n",
        "        # then compute new cell state and hidden state\n",
        "        # see http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
        "        \n",
        "        cell = self.get_cell(layer)\n",
        "        \n",
        "        hidden = self.get_hidden(layer)\n",
        "        \n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "OWo2PU-htb-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 input_size: int, \n",
        "                 hidden_size: int):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "            Dimensionality of the input vector\n",
        "        :param hidden_size: int\n",
        "            Dimensionality of the hidden space\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.cell = LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
        "        \n",
        "    def forward(self, \n",
        "                input: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
        "        \"\"\"\n",
        "        :param input: torch.tensor \n",
        "            Input tesnor for a single observation at timestep t\n",
        "            shape [batch_size, input_size]\n",
        "        Returns Tuple of two torch.tensors, both of shape [seq_len, batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size = input.shape[0]\n",
        "        \n",
        "        hidden, cell = self.init_hidden_cell(batch_size)\n",
        "        \n",
        "        hiddens = []\n",
        "        cells = []\n",
        "        \n",
        "        # this time we will process the whole sequence in the forward method\n",
        "        # as oppose to the previous exercise, remember to loop over the timesteps\n",
        "        \n",
        "        time_steps = input.shape[1]\n",
        "        \n",
        "        \n",
        "        #najlatwiej w pętli\n",
        "        hiddens = torch.zeros(x.size(0), self.hidden_dim)\n",
        "        \n",
        "        cells = torch.zeros(x.size(0), self.hidden_dim)\n",
        "        \n",
        "        return torch.stack(hiddens), torch.stack(cells)\n",
        "    \n",
        "    def init_hidden_cell(self, batch_size):\n",
        "        \"\"\"\n",
        "        Returns initial value for the hidden and cell states\n",
        "        \"\"\"\n",
        "        return (torch.zeros(batch_size, self.hidden_size, requires_grad=True), \n",
        "                torch.zeros(batch_size, self.hidden_size, requires_grad=True))"
      ],
      "metadata": {
        "id": "9esQCKfGtgqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "# torch.manual_seed(1337)\n",
        "\n",
        "# build data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, sampler=sampler)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)\n",
        "\n",
        "# initialize the lstm with an additional cliassifier layer at the top\n",
        "lstm = LSTM(input_size=len(all_letters), hidden_size=128)\n",
        "clf = torch.nn.Linear(in_features=128, out_features=len(label_to_idx))\n",
        "\n",
        "# initialize a optimizer\n",
        "params = chain(lstm.parameters(), clf.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=0.01) \n",
        "\n",
        "# we will train for only a single epoch \n",
        "epoch = 1\n",
        "\n",
        "# main loop\n",
        "for epoch in range(epoch):\n",
        "    \n",
        "    loss_buffer = []\n",
        "    \n",
        "    for i, (x, y) in enumerate(train_loader):   \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # don't forget about the classifier!\n",
        "       \n",
        "        hidden, state = lstm(x)\n",
        "        output = linear_model(i, params, lr=0.01)\n",
        "        \n",
        "        # calucate the loss\n",
        "        loss = cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()                                \n",
        "        \n",
        "        loss_buffer.append(loss.item())\n",
        "        \n",
        "        if i % 1000 == 1:\n",
        "            print(f\"Epoch: {epoch} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\")\n",
        "            loss_buffer = []\n",
        "\n",
        "# evaluate on the test set\n",
        "with torch.no_grad():\n",
        "    \n",
        "    ps = []\n",
        "    ys = []\n",
        "    for i, (x, y) in enumerate(test_loader): \n",
        "        \n",
        "        ys.append(y.numpy())\n",
        "        \n",
        "        hidden, state = lstm(x)\n",
        "        output = model(data)\n",
        "\n",
        "\n",
        "        pred = output.argmax(dim=1)\n",
        "        ps.append(pred.cpu().numpy())\n",
        "    \n",
        "    ps = np.concatenate(ps, axis=0)\n",
        "    ys = np.concatenate(ys, axis=0)\n",
        "    f1 = f1_score(ys, ps, average='weighted')\n",
        "    \n",
        "    print(f\"Final F1 score: {f1:.2f}\")\n",
        "    assert f1 > 0.18, \"You should get over 0.18 f1 score, try changing some hiperparams!\""
      ],
      "metadata": {
        "id": "D0LHuh4qtnm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 4. </p>\n",
        "Zaimplementuj analogiczną do funkcji predict z zadania 2 dla modelu lstm+clf."
      ],
      "metadata": {
        "id": "POd7S63jeVBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_lstm(name: str, lstm: LSTM, clf: torch.nn.Module):\n",
        "    \"\"\"Prints the name and model's top 3 predictions with scores\"\"\"\n",
        "    #poczytałam trochę o funkcji sigmoidalnej, ale nie wiem czy nie zbyt skomplikowane, czy o to chodziło. Pamiętam tylko, że było wspomniane\n",
        "    #z wykorzystaniem LSTM (to można schować, można też przywołać z powyższych)\n",
        "    model = tf.keras.models.Sequential(layers=[\n",
        "    tf.keras.layers.Embedding(input_dim=embedding_input_dim,\n",
        "                            output_dim=embedding_output_dim,\n",
        "                            mask_zero=True),\n",
        "    tf.keras.layers.LSTM(64, return_sequences= True),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(19, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=0.075),\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(sequences, origins_vec_train, epochs=10, batch_size= 200, validation_data= (sequences_test, origins_vec_test))\n",
        "#do wywołania, jak w drugim zadaniu\n",
        "with torch.no_grad():\n",
        "        output = evaluate(lineToTensor(input_line))\n",
        "\n",
        "        topv, topi = output.topk(n_predictions, 1, True)\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(n_predictions):\n",
        "            value = topv[0][i].item()\n",
        "            category_index = topi[0][i].item()\n",
        "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "            predictions.append([value, all_categories[category_index]])\n"
      ],
      "metadata": {
        "id": "3ipIneoWeaWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test your lstm predictor\n",
        "some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n",
        "    \n",
        "for name in some_names:\n",
        "    print(name)\n",
        "    predict_lstm(name, lstm, clf)"
      ],
      "metadata": {
        "id": "48Wy23vHekpE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}